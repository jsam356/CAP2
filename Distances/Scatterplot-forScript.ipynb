{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So far the data is divided in an initial set and a series of additionla batches of data (so far one).\n",
    "\n",
    "#ids will save the ids of the sample (SRAs), projects will save the bioproject identifiers (PJRNA). SRRs will save\n",
    "#the SRR (run) identifier.\n",
    "ids=[]\n",
    "projects=[]\n",
    "SRRs=[]\n",
    "\n",
    "#The ids and corresponding project of the first batch is loaded first from Proj_UID. Proj_UID was generated\n",
    "#in the ProjectMatch notebook. in the Match folder.\n",
    "\n",
    "FirstBatch=open(\"Proj_UID.csv\",\"r\")\n",
    "\n",
    "for line in FirstBatch:\n",
    "    line=line.strip(\"\\n\").split(\";\")\n",
    "    ids.append(line[0])\n",
    "    projects.append(line[1])\n",
    "    SRRs.append(line[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The second batch is loaded in the same way. Proj_UIDBatch2 was generated\n",
    "#in the CheckforNew notebook in the Distances folder.\n",
    "SecondBatch=open(\"Proj_UIDBatch2.csv\",\"r\")\n",
    "\n",
    "for line in SecondBatch:\n",
    "    line=line.strip(\"\\n\").split(\";\")\n",
    "    \n",
    "    if line[0] in ids:\n",
    "        continue\n",
    "    else:\n",
    "        ids.append(line[0])\n",
    "    projects.append(line[1])\n",
    "    SRRs.append(line[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we load the Mash distances. Here they are loaded in the tabular output format of Mash Triangle \n",
    "#generated with the -E flag (if I'm not wrong).\n",
    "import numpy as np\n",
    "\n",
    "phy=open(\"NewDistances.tab\",\"r\")\n",
    "\n",
    "#The distances are saved in a matrix. The dimensions of the matrix can be found by determining the length\n",
    "#of ids.\n",
    "distances=np.zeros((1617,1617))\n",
    "\n",
    "#At each line the tab file is parsed and the distances added. The file includes, amongst others, the \n",
    "#id of all pairs of samples and their distance. We parse the ids and distance and place the distance in\n",
    "#the corresponding spot in the distance matrix, so that the order of samples in the list ids corresponds \n",
    "#to the distances in the matrix.\n",
    "for line in phy:\n",
    "    line=line.strip(\"\\n\").split(\"\\t\")\n",
    "    id1=line[0].split(\"_\")[0].split(\"/\")[1]\n",
    "    id2=line[1].split(\"_\")[0].split(\"/\")[1]\n",
    "    dist=float(line[2])\n",
    "    distances[ids.index(id1),ids.index(id2)]=dist\n",
    "    distances[ids.index(id2),ids.index(id1)]=dist\n",
    "phy.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776\n"
     ]
    }
   ],
   "source": [
    "#Hereon we're interested in a dereplicated list of all projects, so we dereplicate projects ans save as\n",
    "#Ordprojects (for now).\n",
    "Ordprojects=list(set(projects))\n",
    "print(len(Ordprojects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We generate col, a list which will map every sample to a number, and that number to a given project, based\n",
    "#on the order of appareance of the projects in Proj_UID and Proj_UIDBatch2. More on col below.\n",
    "col=[]\n",
    "\n",
    "for i in projects:\n",
    "    col.append(Ordprojects.index(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The original ordered project list, with the ith positions having the project of the ith id, is saved in\n",
    "#UnOrdprojects, while the dereplicated list is now saved simply as projects.\n",
    "UnOrdprojects=projects\n",
    "projects=Ordprojects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we move on to load the keywords and MeSH terms for the bioprojects which have them. In general almost\n",
    "#all bioprojects have keywords, while only a handful have MeSH Terms.\n",
    "\n",
    "#For keywords the file Papers-Grid_Min.csv (custom made based on the Airtable data) contains the title of the\n",
    "#project (or paper if there's a paper), the bioproject identifier and the keywords from airtable.\n",
    "fpapers=open(\"Papers-Grid_Min.csv\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "#We'll save the keywords per project in keywords and the projects corresponding to each set of keywords in\n",
    "#ProjforKeywords. Note that we're mapping projects to keywords, not samples to keywords. Also, since \n",
    "#Papers-Grid_Min.csv was downloaded from Airtable, the order of the projects is not necessarily the same as \n",
    "#in the list projects generated above (that's why we need another list).\n",
    "ProjforKeywords=[]\n",
    "keywords=[]\n",
    "\n",
    "for line in fpapers:\n",
    "#We use the PRJNA in the project id as a separator and get the list of keywords from every project.\n",
    "    if \"PRJNA\" in line:\n",
    "        line=line.strip(\"\\n\").split('PRJNA')\n",
    "        projectID=\"PRJNA\"+line[1].split(\",\")[0].split(\";\")[0].split(\".pdf\")[0].split(\")\")[0]\n",
    "\n",
    "#Since we're only interested in the keywords of the projects whose samples have Mash distances, we filter\n",
    "#the projects and get the keywords of those which are present in Proj_UID.csv or Proj_UIDBatch2.csv.\n",
    "        if projectID in projects:\n",
    "            ProjforKeywords.append(projectID)\n",
    "            Prelkeywords=line[1].split(\",\")[1::]\n",
    "            Realkeywords=[]\n",
    "            for i in Prelkeywords:\n",
    "                if i==\"\":\n",
    "                    continue\n",
    "                else:\n",
    "#keywords are added in lowercase.\n",
    "                    Realkeywords.append(i.lower())\n",
    "            keywords.append(Realkeywords)\n",
    "fpapers.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we define a dictionary which will store the projects (value) associated to each keyword (key) present\n",
    "#in the dataset. \n",
    "kwordtoProj={}\n",
    "\n",
    "for i in range(len(keywords)):\n",
    "    for j in keywords[i]:\n",
    "        if j in kwordtoProj.keys():\n",
    "            kwordtoProj[j].append(ProjforKeywords[i])\n",
    "        else:\n",
    "             kwordtoProj[j]=[ProjforKeywords[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For MeSH terms we employ the file MeSHTerms.csv which contains the major and minor MeSH terms per \n",
    "#article and project. This csv is partly made with notebook Title_Description_Extraction.csv in \n",
    "#Match/Keywords_MeSH, and partly done manually.\n",
    "\n",
    "MeShTerms=open(\"MeSHTerms.csv\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "#We generate lists to hold the major and minor MeSH terms per bioproject (MeSHMaj and MeSHMin), and the \n",
    "#bioprojects which have them in the order they are found in MeSHTerms.csv\n",
    "ProjforMeSH=[]\n",
    "MesHMaj=[]\n",
    "MesHMin=[]\n",
    "\n",
    "#These two are temporary lists for holding the terms associated to single bioprojects.\n",
    "thisMaj=[]\n",
    "tempMin=[]\n",
    "\n",
    "#Due to the way the csv with the MeSH terms is structured, a variable with the first project id in the file\n",
    "#needs to be defined.\n",
    "Currentproject=\"PRJNA450123\"\n",
    "\n",
    "#Now the meSH terms are loaded.\n",
    "while True:\n",
    "    line = MeShTerms.readline()\n",
    "    if line is None or line=='':\n",
    "        break\n",
    "    else:\n",
    "#The file has one line per Major-Minor MeSh term combination in each specific project, so there are many\n",
    "#lines associated to each project.\n",
    "        line=line.strip(\"\\n\").split(\";\")\n",
    "        if line[1]==Currentproject:\n",
    "            thisMaj.append(line[2])\n",
    "            thisMin=[]\n",
    "#While the same bioproject appears in the following line, we add Major and Minor terms into temporary lists.\n",
    "            for i in range(3,len(line)):\n",
    "                if line[i]!=\"\":\n",
    "                    thisMin.append(line[i])\n",
    "            tempMin.append(thisMin)\n",
    "#When the bioproject changes we save the terms of the prior bioproject in their corresponding lists.\n",
    "        else:\n",
    "            MesHMaj.append(thisMaj)\n",
    "            MesHMin.append(tempMin)\n",
    "            Currentproject=line[1]\n",
    "            ProjforMeSH.append(line[1])\n",
    "            thisMaj=[line[2]]\n",
    "            tempMin=[]\n",
    "            thisMin=[]\n",
    "            for i in range(3,len(line)):\n",
    "                if line[i]!=\"\":\n",
    "                    thisMin.append(line[i])\n",
    "            tempMin.append(thisMin)\n",
    "    \n",
    "MeShTerms.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We generate dictionaries with the Major and Minor MeSH terms (key) and the bioprojects the're included\n",
    "#in (value).\n",
    "\n",
    "#Note that for minor MeSH terms we're considering the combination of the major and the minor term as a key,\n",
    "#to avoid ambiguities.\n",
    "MajMeSHtoProj={}\n",
    "MinMeSHtoProj={}\n",
    "\n",
    "for i in range(len(MesHMaj)):\n",
    "    for j in range(len(MesHMaj[i])):\n",
    "        if MesHMaj[i][j] in MajMeSHtoProj.keys():\n",
    "            MajMeSHtoProj[MesHMaj[i][j]].append(ProjforMeSH[i])\n",
    "        else:\n",
    "             MajMeSHtoProj[MesHMaj[i][j]]=[ProjforMeSH[i]]\n",
    "#Major and minor MeSH combinations are separated by _ to make them easily distinguishable.\n",
    "        for z in MesHMin[i][j]:\n",
    "            if MesHMaj[i][j]+\"_\"+z in MinMeSHtoProj.keys():\n",
    "                MinMeSHtoProj[MesHMaj[i][j]+\"_\"+z].append(ProjforMeSH[i])\n",
    "            else:\n",
    "                MinMeSHtoProj[MesHMaj[i][j]+\"_\"+z]=[ProjforMeSH[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some of the projects we're working with have abstracts in Airtable which might contain the keywords which\n",
    "#are associated to the projects. Therefore, it makes sense to look for keywords in the abstracts of the \n",
    "#projects, to make sure that all bioprojects which have a given keyword in their abstracts (but not in their\n",
    "#original set of keywords) now have that keyword.\n",
    "\n",
    "#Note that abstracts in Airtable might be shorts descriptions or paper abstracts per se, depending\n",
    "#on the case.\n",
    "\n",
    "#The abstracts are saved in Abstracts.csv, which contains the title of the project, the project ID, and the\n",
    "#abstract itself.\n",
    "abst=open(\"Abstracts.csv\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "while True:\n",
    "    line = abst.readline()\n",
    "    if line is None or line=='':\n",
    "        break\n",
    "#Due to the way the Airtable information must be downloaded, abstracts can span multiple lines in the file.\n",
    "#We look for the PJRNA in the bioproject identifier and use that to identify the beggining of an abstract\n",
    "#and save all its lines.\n",
    "    else:\n",
    "        if \"PRJNA\" in line:\n",
    "            line=line.strip(\"\\n\").split(\"PRJNA\")\n",
    "            projectID=\"PRJNA\"+line[1].split(\",\")[0].split(\";\")[0].split(\".pdf\")[0].split(\")\")[0]\n",
    "            #print(projectID)\n",
    "#Naturally we only consider projects whose samples have Mash distances.\n",
    "            if projectID in projects:\n",
    "                try:\n",
    "#Some of the abstracts begin with double quotes, which are used to mark the beggining and end of them, so \n",
    "#they're used to parse them and mark the end of the abstract.\n",
    "                    text=line[1].split('\"')[-2]\n",
    "                    while '\"' not in text:\n",
    "#When an abstract is found all its line are searched for the presence of any of the keywords. if any its find\n",
    "#and its not already associated to the bioproject, the bioproject is added to the list of projects in the \n",
    "#keyword dictionary, and the keyword added to the list of keywords for that bioproject.\n",
    "                        for j in kwordtoProj.keys():\n",
    "                            if j in text.lower() and projectID not in kwordtoProj[j]:\n",
    "                                kwordtoProj[j].append(projectID)\n",
    "                                keywords[ProjforKeywords.index(projectID)].append(j)\n",
    "                        text=abst.readline().strip(\"\\n\")\n",
    "#For abstracts without double quotes we use commas.\n",
    "                except IndexError:\n",
    "                    text=line[1].split(',')[-1]\n",
    "                    for j in kwordtoProj.keys():\n",
    "                            if j in text.lower() and projectID not in kwordtoProj[j]:\n",
    "                                kwordtoProj[j].append(projectID)\n",
    "                                keywords[ProjforKeywords.index(projectID)].append(j)\n",
    "abst.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we move on to consolidate keywords if necessary. Consolidate means group together keywords which directly \n",
    "#mean the same thing (Permanent or Primary Consolidation) and those which are related (Secondary Consolidation).\n",
    "\n",
    "#Both consolidations are given by two csv files which are custom made: PermanentConsolidation.csv and \n",
    "#SecondaryConsolidation.csv. They contain in the first column the word in which the keywords will be consolidated \n",
    "#(consolidated term), and in the rest of the columns the keywords already present which will be consolidated \n",
    "#into the word in the first columns.\n",
    "\n",
    "#We do the same procedure for both files. First we open them.\n",
    "PC=open(\"PermanentConsolidation.csv\",\"r\")\n",
    "\n",
    "#Then a dictionary is defined with the keys being the word to consolidate and the values lists of the present\n",
    "#keywords to be consolidated into that word.\n",
    "FirstConsolidate={}\n",
    "\n",
    "for line in PC:\n",
    "    line=line.strip(\"\\n\").split(\";\")\n",
    "    \n",
    "#The keywords present are appended in lower case to coincide with the ones in kwordtoProj.\n",
    "    thisList=[]\n",
    "    for i in line[1::]:\n",
    "        if i!=\"\":\n",
    "            thisList.append(i.lower())\n",
    "    FirstConsolidate[line[0]]=thisList\n",
    "\n",
    "PC.close()\n",
    "\n",
    "#The secondary consolidate file is parsed in the same way.\n",
    "SC=open(\"SecondaryConsolidation.csv\",\"r\")\n",
    "\n",
    "SecondaryConsolidate={}\n",
    "\n",
    "for line in SC:\n",
    "    line=line.strip(\"\\n\").split(\";\")\n",
    "    \n",
    "    thisList=[]\n",
    "    for i in line[1::]:\n",
    "        if i!=\"\":\n",
    "            thisList.append(i.lower())\n",
    "#There are two terms which need to be handled uniquely due to they being consolidated terms for the first consolidation\n",
    "#but being then consolidated into other term in the second consolidation.\n",
    "        if \"cabbage looper\" in thisList:\n",
    "            thisList[thisList.index(\"cabbage looper\")]=\"Cabbage looper\"\n",
    "        if \"16s\" in thisList:\n",
    "            thisList[thisList.index(\"16s\")]=\"16S\"\n",
    "    SecondaryConsolidate[line[0]]=thisList\n",
    "\n",
    "SC.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function generate consolidation takes the dictionaries loaded from either PermanentConsolidation.csv or \n",
    "#SecondaryConsolidation.csv and updates kwordtoProj, keywords and ProjforKeywords so that the presence of the\n",
    "#consolidated terms in the projects now shown, and the terms consolidated are erased from those objects.\n",
    "\n",
    "#For the sake of generality, kwordtoProj, keywords and ProjforKeywords are not directly used in the function \n",
    "#definition, but they should be pased as the parameters wordDict,ProjWordList, and ProjtoWord.\n",
    "def generateConsolidation(ConsolidateDict,wordDict,ProjWordList,ProjtoWord):\n",
    "#The function first adds to wordDict the new consolidated terms as keys and the list of the bioprojects they're in\n",
    "#as values. The consolidated terms are also added to the ordered list with keywords per bioproject. The terms to \n",
    "#be consolidated are added to the list toRemove.\n",
    "    toRemove=[]\n",
    "    for NewTerm in ConsolidateDict.keys():\n",
    "        thisList=[]\n",
    "        for OldTerm in ConsolidateDict[NewTerm]:\n",
    "            thisList.extend(wordDict[OldTerm])\n",
    "            toRemove.append(OldTerm)\n",
    "        wordDict[NewTerm]=list(set(thisList))\n",
    "#Here the consolidated terms are added to the ordered list of keywords per bioproject, and the terms to be consolidated\n",
    "#are removed from it.\n",
    "    for proj in ProjtoWord:\n",
    "        for NewTerm in ConsolidateDict.keys():\n",
    "            if proj in wordDict[NewTerm]:\n",
    "                ProjWordList[ProjtoWord.index(proj)].append(NewTerm)\n",
    "        for OldTerm in toRemove:\n",
    "            if proj in wordDict[OldTerm]:\n",
    "                while ProjWordList[ProjtoWord.index(proj)].count(OldTerm)!=0:\n",
    "                    ProjWordList[ProjtoWord.index(proj)].remove(OldTerm)\n",
    "#Finally, the terms to be consolidated are removed from the dictionary of keywords.\n",
    "    for key in toRemove:\n",
    "        try:\n",
    "            del wordDict[key]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We run the first consolidation.\n",
    "generateConsolidation(FirstConsolidate,kwordtoProj,keywords,ProjforKeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we move to the process of generating coloring schemes for graphing. Earlier we defined col, an ordered list\n",
    "#in which each sample is assigned a number which corresponds to the position of its corresponding bioproject in\n",
    "#the bioprojects list. That will be useful for coloring per project in the graphs. However, we might want to\n",
    "#color by the presence of specific terms.\n",
    "\n",
    "#change_col generates coloring schemes based on the presence of keywords or MeSH terms. dictOne refers to a MeSh or\n",
    "#keyword dictionary, words the terms to be used for the coloring shceme, and all_col whether we should consider\n",
    "#simultaneous presence of different terms in single samples (all_col=1) or not (all_col=0).\n",
    "def change_col(dictOne,words,all_col=0):\n",
    "#First a number is assigned to every term. The range of the number can be changed by changing the value of\n",
    "#z and the line where z number is added to z. The idea of first using 20 and adding 10 is to increase the spread\n",
    "#of the values for having a more divergent coloring in matplotlib.\n",
    "\n",
    "#colmap maps terms to numbers, and numMap numbers to terms.\n",
    "    colmap={}\n",
    "    numMap={}\n",
    "    z=20\n",
    "    for j in words:\n",
    "        colmap[j]=z\n",
    "        numMap[z]=j\n",
    "        z+=10\n",
    "#After the colors are assigned and saved in colmap and numMap then each sample is is checked (using cols to locate\n",
    "#its project) to see if it harbors the term in question. \n",
    "    new_cols=[]\n",
    "    for i in range(len(col)):\n",
    "        proj=projects[col[i]]\n",
    "#If all_col=1 then all terms are checked and the number associated to the sample is the average of the terms\n",
    "#which its bioproject contains. The term itself will be a combination of all the terms present. \n",
    "        if all_col==1:\n",
    "            this_col=[]\n",
    "            thisTerm=\"\"\n",
    "            for j in words:\n",
    "                if proj in dictOne[j]:\n",
    "                    if thisTerm==\"\":\n",
    "                        thisTerm=j\n",
    "                    else:\n",
    "                        thisTerm=thisTerm+\" & \"+j\n",
    "                    this_col.append(colmap[j])\n",
    "            if thisTerm==\"\":\n",
    "#The samples with no terms are assigned 2 as an arbitrary value (this can be changed).\n",
    "                new_cols.append(2)\n",
    "                continue\n",
    "            elif thisTerm not in colmap.keys():\n",
    "                colmap[thisTerm]=[mean(this_col)]\n",
    "                numMap[mean(this_col)]=thisTerm\n",
    "            new_cols.append(mean(this_col))\n",
    "#If all_col=0 then the sample is assigned the first term it has from the ones considered.\n",
    "        else:\n",
    "            found=0\n",
    "            for j in words:\n",
    "                if proj in dictOne[j]:\n",
    "                    new_cols.append(colmap[j])\n",
    "                    found=1\n",
    "                    break\n",
    "            if found==0:\n",
    "                new_cols.append(2)\n",
    "#The function returns the dictionaries mapping colors and terms and the ordered list of new numbers (colors)                \n",
    "#for each sample.\n",
    "    return new_cols,colmap,numMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for Significant Differences in Clustering for Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can do a more individualized and comprehensive test for each of the keywords. We'll consider each keyword \n",
    "#individually and perform three tests. \n",
    "import statistics \n",
    "import random\n",
    "import copy\n",
    "\n",
    "#One of the tests is going to be an aleatorization analysis. For that we'll define a function to perform an \n",
    "#aleatorizations. We'll need the distance matrix (dist), the number of times the aleatorization should take place\n",
    "#(times) and the labels of the groups (labels).\n",
    "def aleatorizationForTerms(dist,times,labels):\n",
    "    differences=[]\n",
    "    newlabels=copy.deepcopy(labels)\n",
    "#What we do is shuffle the list of labels the number of times indicated by times.\n",
    "    for i in range(0,times):\n",
    "        random.shuffle(newlabels)\n",
    "        group_distances=[]\n",
    "        diff_distances=[]\n",
    "#With the shuffling done we get the distances of samples with the term and without the term.\n",
    "        for i in range(1,dist.shape[0]):\n",
    "            for j in range(0,i):\n",
    "                if newlabels[i]==20 and newlabels[j]==20:\n",
    "                    group_distances.append(dist[i,j])\n",
    "                elif newlabels[i]==2 and newlabels[j]==2:\n",
    "                    diff_distances.append(dist[i,j])\n",
    "#We compute each tieme the difference of the medians of the distance of both groups (with the term and without it) \n",
    "#based on the labels.\n",
    "        differences.append(statistics.median(diff_distances)-statistics.median(group_distances))\n",
    "#At the end the function returns a list with all the results of this median difference.\n",
    "    differences.sort()\n",
    "    return differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biodiversity\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-5a85d8089906>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mactual_diff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff_distances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_distances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m#After that we get the result of the aleatorization and compare with the median distribution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mRanddiff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maleatorizationForTerms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mncol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRanddiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual_diff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-f8921ccaabe7>\u001b[0m in \u001b[0;36maleatorizationForTerms\u001b[1;34m(dist, times, labels)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#We compute each tieme the difference of the medians of the distance between and within samples based on the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m#labels.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mdifferences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff_distances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_distances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;31m#At the end the function returns a list with all the results of this median difference.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mdifferences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\statistics.py\u001b[0m in \u001b[0;36mmedian\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    426\u001b[0m     \"\"\"\n\u001b[0;32m    427\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mStatisticsError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"no median for empty data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Now we do the tests themselves. We'll do the same two tests done in the previous analyses (Wilcoxon and Kolmogorov-\n",
    "#Smirnov) apart from the aleatorization. \n",
    "from scipy.stats import ranksums,kstest\n",
    "significant=[]\n",
    "abSignificant=[]\n",
    "abundance=[]\n",
    "\n",
    "#Running aleatorizations for all the keywords would be particularly costly computationally, so we establish an \n",
    "#abundance cut. This cut can be set based on the overall abundance of keywords per projects or samples. \n",
    "#A histogram of the abundance in bioprojects or samples of the keywords is shown below.\n",
    "byProj=0\n",
    "\n",
    "for term in kwordtoProj.keys():\n",
    "#If we'll work with the abundance by bioproject then we set byProj=1 and estimate abundances based on this.\n",
    "#If we set byProj=0 then we count abundances by sample and filter using samples. Note that regardless of this\n",
    "#the distance analysis is always done using samples.\n",
    "\n",
    "#The default cut is 5, but this can be changed here. The list abundances will save the abundance of all keywords\n",
    "#in terms of samples or bioprojects.\n",
    "    if byProj==1:\n",
    "        abundance.append(len(kwordtoProj[term]))\n",
    "        if len(kwordtoProj[term])<5:\n",
    "            continue\n",
    "        ncol,thisColMap,Wol=change_col(kwordtoProj,[term],0)\n",
    "    else:\n",
    "        ncol,thisColMap,Wol=change_col(kwordtoProj,[term],0)\n",
    "        abundance.append(ncol.count(20))\n",
    "        if abundance[-1]>25 or abundance[-1]<20:\n",
    "            continue\n",
    "    print(term)\n",
    "\n",
    "#For each term we get the distances of samples which have the keyword or don't as we've done before.\n",
    "    group_distances=[]\n",
    "    diff_distances=[]\n",
    "    for i in range(1,distances.shape[0]):\n",
    "        for j in range(0,i):\n",
    "            if ncol[i]==20  and ncol[j]==20:\n",
    "                group_distances.append(distances[i,j])\n",
    "            elif ncol[i]==2  and ncol[j]==2:\n",
    "                diff_distances.append(distances[i,j])\n",
    "\n",
    "#We then move on to carry out the three tests. First the Wilcoxon and Kolmogorov-Smirnov.\n",
    "    a,b=ranksums(group_distances,diff_distances)\n",
    "    c,d=kstest(group_distances,diff_distances)\n",
    "#Then we get the real difference of medians between samples with the same term and different terms.\n",
    "    actual_diff=statistics.median(diff_distances)-statistics.median(group_distances)\n",
    "    #After that we get the result of the aleatorization and compare with the median distribution.\n",
    "    Randdiff=aleatorizationForTerms(distances,1000,ncol)\n",
    "    SignPosLow=int((len(Randdiff)*0.01))\n",
    "    SignPosHigh=int((len(Randdiff)*0.99))\n",
    "#We compare the p-values of the first two tests (with bonferroni correction) and check the aleatorization. For the\n",
    "#aleatorization we check if the distances is smaller than the 1% percentile of the random median distance distribution.\n",
    "    if b<0.01/(2*len(kwordtoProj)) and d<0.01/(2*len(kwordtoProj)) and (Randdiff[SignPosLow]>actual_diff or Randdiff[SignPosHigh]<actual_diff):\n",
    "#We print the terms which pass the criteria with their abundance\n",
    "        if Randdiff[SignPosLow]>actual_diff:\n",
    "            where=\"low\"\n",
    "        else:\n",
    "            where=\"high\"\n",
    "        if byProj==1:\n",
    "            abSignificant.append(len(kwordtoProj[term]))\n",
    "            print(term+\"\\t\"+str(len(kwordtoProj[term]))+\"\\t\"+where)\n",
    "        else:\n",
    "            abSignificant.append(abundance[-1])  \n",
    "            print(term+\"\\t\"+str(abundance[-1])+\"\\t\"+where)\n",
    "        significant.append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we run the same test but with MajorMeSH terms. The process is exactly the same, what changes are some variable\n",
    "#names and the dictionary we're using. The comments are basically the same as above with minor changes.\n",
    "from scipy.stats import ranksums,kstest\n",
    "MeSHsignificant=[]\n",
    "MeSHabSignificant=[]\n",
    "MeSHabundance=[]\n",
    "\n",
    "#Running aleatorizations for all the Major MeSH would be particularly costly computationally, so we establish an \n",
    "#abundance cut. This cut can be set based on the overall abundance of Major MeSH per projects or samples. \n",
    "#A histogram of the abundance in bioprojects or samples of the Major MeSH is shown below.\n",
    "byProj=0\n",
    "\n",
    "for term in MajMeSHtoProj.keys():\n",
    "#If we'll work with the abundance by bioproject then we set byProj=1 and estimate abundances based on this.\n",
    "#If we set byProj=0 then we count abundances by sample and filter using samples. Note that regardless of this\n",
    "#the distance analysis is always done using samples.\n",
    "\n",
    "#The default cut is 5, but this can be changed here. The list MeSHabundances will save the abundance of all Major MeSH\n",
    "#in terms of samples or bioprojects.\n",
    "    if byProj==1:\n",
    "        MeSHabundance.append(len(MajMeSHtoProj[term]))\n",
    "        if len(MajMeSHtoProj[term])<5:\n",
    "            continue\n",
    "        ncol,thisColMap,Wol=change_col(MajMeSHtoProj,[term],0)\n",
    "    else:\n",
    "        ncol,thisColMap,Wol=change_col(MajMeSHtoProj,[term],0)\n",
    "        MeSHabundance.append(ncol.count(20))\n",
    "        if MeSHabundance[-1]<5:\n",
    "            continue\n",
    "    print(term)\n",
    "\n",
    "#For each term we get the distances of samples which have the Major MeSH or don't as we've done before.\n",
    "    group_distances=[]\n",
    "    diff_distances=[]\n",
    "\n",
    "    for i in range(1,distances.shape[0]):\n",
    "        for j in range(0,i):\n",
    "            if ncol[i]==20 and ncol[j]==20:\n",
    "                group_distances.append(distances[i,j])\n",
    "            elif ncol[i]==2 and ncol[j]==2:\n",
    "                diff_distances.append(distances[i,j])\n",
    "\n",
    "#We then move on to carry out the three tests. First the Wilcoxon and Kolmogorov-Smirnov.\n",
    "    a,b=ranksums(group_distances,diff_distances)\n",
    "    c,d=kstest(group_distances,diff_distances)\n",
    "#Then we get the real difference of medians between samples with the same term and different terms.\n",
    "    actual_diff=statistics.median(diff_distances)-statistics.median(group_distances)\n",
    "    #After that we get the result of the aleatorization and compare with the median distribution.\n",
    "    Randdiff=aleatorizationForTerms(distances,1000,ncol)\n",
    "    SignPosLow=int((len(Randdiff)*0.01))\n",
    "    SignPosHigh=int((len(Randdiff)*0.99))\n",
    "#We compare the p-values of the first two tests (with bonferroni correction) and check the aleatorization. For the\n",
    "#aleatorization we check if the distances is smaller than the 1% percentile of the random median distance distribution.\n",
    "    if b<0.01/(2*len(MajMeSHtoProj)) and d<0.01/(2*len(MajMeSHtoProj)) and (Randdiff[SignPosLow]>actual_diff or Randdiff[SignPosHigh]<actual_diff):\n",
    "#We print the terms which pass the criteria with their abundance\n",
    "        if Randdiff[SignPosLow]>actual_diff:\n",
    "            where=\"low\"\n",
    "        else:\n",
    "            where=\"high\"\n",
    "        if byProj==1:\n",
    "            MeSHabSignificant.append(len(MeSHabSignificant[term]))\n",
    "            print(term+\"\\t\"+str(len(MajMeSHtoProj[term])))\n",
    "        else:\n",
    "            MeSHabSignificant.append(MeSHabundance[-1]) \n",
    "            print(term+\"\\t\"+str(MeSHabundance[-1]))\n",
    "        MeSHsignificant.append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ascomycota_metabolism\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-5ad63c0ee615>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m#We then move on to carry out the three tests. First the Wilcoxon and Kolmogorov-Smirnov.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mranksums\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_distances\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiff_distances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkstest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_distances\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiff_distances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;31m#Then we get the real difference of medians between samples with the same term and different terms.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mactual_diff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff_distances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_distances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py\u001b[0m in \u001b[0;36mkstest\u001b[1;34m(rvs, cdf, args, N, alternative, mode)\u001b[0m\n\u001b[0;32m   6893\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcdf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6894\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mks_1samp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malternative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malternative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6895\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mks_2samp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malternative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malternative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py\u001b[0m in \u001b[0;36mks_2samp\u001b[1;34m(data1, data2, alternative, mode)\u001b[0m\n\u001b[0;32m   6687\u001b[0m     \u001b[1;31m# using searchsorted solves equal data problem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6688\u001b[0m     \u001b[0mcdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mside\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'right'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6689\u001b[1;33m     \u001b[0mcdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mside\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'right'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6690\u001b[0m     \u001b[0mcddiffs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcdf1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcdf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6691\u001b[0m     \u001b[0mminS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcddiffs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Ensure sign of minS is not negative.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Now with Minor MeSH terms. \n",
    "from scipy.stats import ranksums,kstest\n",
    "MinMeSHsignificant=[]\n",
    "MinMeSHabSignificant=[]\n",
    "MinMeSHabundance=[]\n",
    "\n",
    "#Running aleatorizations for all the Minor MeSH would be particularly costly computationally, so we establish an \n",
    "#abundance cut. This cut can be set based on the overall abundance of Major MeSH per projects or samples. \n",
    "#A histogram of the abundance in bioprojects or samples of the Major MeSH is shown below.\n",
    "byProj=0\n",
    "\n",
    "for term in MinMeSHtoProj.keys():\n",
    "#If we'll work with the abundance by bioproject then we set byProj=1 and estimate abundances based on this.\n",
    "#If we set byProj=0 then we count abundances by sample and filter using samples. Note that regardless of this\n",
    "#the distance analysis is always done using samples.\n",
    "\n",
    "#The default cut is 5, but this can be changed here. The list MinMeSHabundance will save the abundance of all Minor \n",
    "#MeSH in terms of samples or bioprojects.\n",
    "    if byProj==1:\n",
    "        MinMeSHabundance.append(len(MinMeSHtoProj[term]))\n",
    "        if len(MinMeSHtoProj[term])<5:\n",
    "            continue\n",
    "        ncol,thisColMap,Wol=change_col(MinMeSHtoProj,[term],0)\n",
    "    else:\n",
    "        ncol,thisColMap,Wol=change_col(MinMeSHtoProj,[term],0)\n",
    "        MinMeSHabundance.append(ncol.count(20))\n",
    "        if MinMeSHabundance[-1]<5:\n",
    "            continue\n",
    "    print(term)\n",
    "\n",
    "#For each term we get the distances of samples which have the Major MeSH or don't as we've done before.\n",
    "    group_distances=[]\n",
    "    diff_distances=[]\n",
    "\n",
    "    for i in range(1,distances.shape[0]):\n",
    "        for j in range(0,i):\n",
    "            if ncol[i]==20 and ncol[j]==20:\n",
    "                group_distances.append(distances[i,j])\n",
    "            elif ncol[i]==2 and ncol[j]==2:\n",
    "                diff_distances.append(distances[i,j])\n",
    "\n",
    "#We then move on to carry out the three tests. First the Wilcoxon and Kolmogorov-Smirnov.\n",
    "    a,b=ranksums(group_distances,diff_distances)\n",
    "    c,d=kstest(group_distances,diff_distances)\n",
    "#Then we get the real difference of medians between samples with the same term and different terms.\n",
    "    actual_diff=statistics.median(diff_distances)-statistics.median(group_distances)\n",
    "    #After that we get the result of the aleatorization and compare with the median distribution.\n",
    "    Randdiff=aleatorizationForTerms(distances,1000,ncol)\n",
    "    SignPosLow=int((len(Randdiff)*0.01))\n",
    "    SignPosHigh=int((len(Randdiff)*0.99))\n",
    "#We compare the p-values of the first two tests (with bonferroni correction) and check the aleatorization. For the\n",
    "#aleatorization we check if the distances is smaller than the 1% percentile of the random median distance distribution.\n",
    "    if b<0.01/(2*len(MinMeSHtoProj)) and d<0.01/(2*len(MinMeSHtoProj)) (Randdiff[SignPosLow]>actual_diff or Randdiff[SignPosHigh]<actual_diff):\n",
    "#We print the terms which pass the criteria with their abundance.\n",
    "        if Randdiff[SignPosLow]>actual_diff:\n",
    "            where=\"low\"\n",
    "        else:\n",
    "            where=\"high\"\n",
    "        if byProj==1:\n",
    "            MinMeSHabSignificant.append(len(MinMeSHtoProj[term]))\n",
    "            print(term+\"\\t\"+str(len(MinMeSHtoProj[term])))\n",
    "        else:\n",
    "            MinMeSHabSignificant.append(MinMeSHabundance[-1]) \n",
    "            print(term+\"\\t\"+str(MinMeSHtoProj[-1]))\n",
    "        MinMeSHsignificant.append(term)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
